{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allstate Claims Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "#import sklearn.cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"../input/train.csv\"\n",
    "TRAIN_PATH = \"/Users/lordlavon/Desktop/train.csv\"\n",
    "\n",
    "# \"../input/test.csv\"\n",
    "TEST_PATH = \"/Users/lordlavon/Desktop/test.csv\"\n",
    "\n",
    "SPLIT = 116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH) \n",
    "test = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save ids for submission file\n",
    "ids = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
      "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
      "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
      "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
      "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
      "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
      "\n",
      "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
      "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
      "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
      "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
      "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
      "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
      "\n",
      "     cont14     loss  \n",
      "0  0.714843  2213.18  \n",
      "1  0.304496  1283.60  \n",
      "2  0.774425  3005.09  \n",
      "3  0.602642   939.85  \n",
      "4  0.432606  2763.85  \n",
      "\n",
      "[5 rows x 132 columns]\n"
     ]
    }
   ],
   "source": [
    "# take a look at training data\n",
    "print(train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
      "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
      "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
      "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
      "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
      "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
      "\n",
      "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
      "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
      "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
      "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
      "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
      "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
      "\n",
      "     cont13    cont14  \n",
      "0  0.704052  0.392562  \n",
      "1  0.453468  0.208045  \n",
      "2  0.258586  0.297232  \n",
      "3  0.592264  0.555955  \n",
      "4  0.301535  0.825823  \n",
      "\n",
      "[5 rows x 131 columns]\n"
     ]
    }
   ],
   "source": [
    "# take a look at testing data\n",
    "print(test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training data is: \n",
      "(188318, 132)\n",
      "Description of training data is: \n",
      "                  id          cont1          cont2          cont3  \\\n",
      "count  188318.000000  188318.000000  188318.000000  188318.000000   \n",
      "mean   294135.982561       0.493861       0.507188       0.498918   \n",
      "std    169336.084867       0.187640       0.207202       0.202105   \n",
      "min         1.000000       0.000016       0.001149       0.002634   \n",
      "25%    147748.250000       0.346090       0.358319       0.336963   \n",
      "50%    294539.500000       0.475784       0.555782       0.527991   \n",
      "75%    440680.500000       0.623912       0.681761       0.634224   \n",
      "max    587633.000000       0.984975       0.862654       0.944251   \n",
      "\n",
      "               cont4          cont5          cont6          cont7  \\\n",
      "count  188318.000000  188318.000000  188318.000000  188318.000000   \n",
      "mean        0.491812       0.487428       0.490945       0.484970   \n",
      "std         0.211292       0.209027       0.205273       0.178450   \n",
      "min         0.176921       0.281143       0.012683       0.069503   \n",
      "25%         0.327354       0.281143       0.336105       0.350175   \n",
      "50%         0.452887       0.422268       0.440945       0.438285   \n",
      "75%         0.652072       0.643315       0.655021       0.591045   \n",
      "max         0.954297       0.983674       0.997162       1.000000   \n",
      "\n",
      "               cont8          cont9         cont10         cont11  \\\n",
      "count  188318.000000  188318.000000  188318.000000  188318.000000   \n",
      "mean        0.486437       0.485506       0.498066       0.493511   \n",
      "std         0.199370       0.181660       0.185877       0.209737   \n",
      "min         0.236880       0.000080       0.000000       0.035321   \n",
      "25%         0.312800       0.358970       0.364580       0.310961   \n",
      "50%         0.441060       0.441450       0.461190       0.457203   \n",
      "75%         0.623580       0.566820       0.614590       0.678924   \n",
      "max         0.980200       0.995400       0.994980       0.998742   \n",
      "\n",
      "              cont12         cont13         cont14           loss  \n",
      "count  188318.000000  188318.000000  188318.000000  188318.000000  \n",
      "mean        0.493150       0.493138       0.495717    3037.337686  \n",
      "std         0.209427       0.212777       0.222488    2904.086186  \n",
      "min         0.036232       0.000228       0.179722       0.670000  \n",
      "25%         0.311661       0.315758       0.294610    1204.460000  \n",
      "50%         0.462286       0.363547       0.407403    2115.570000  \n",
      "75%         0.675759       0.689974       0.724623    3864.045000  \n",
      "max         0.998484       0.988494       0.844848  121012.250000  \n",
      "Skewness of training data is: \n",
      "id       -0.002155\n",
      "cont1     0.516424\n",
      "cont2    -0.310941\n",
      "cont3    -0.010002\n",
      "cont4     0.416096\n",
      "cont5     0.681622\n",
      "cont6     0.461214\n",
      "cont7     0.826053\n",
      "cont8     0.676634\n",
      "cont9     1.072429\n",
      "cont10    0.355001\n",
      "cont11    0.280821\n",
      "cont12    0.291992\n",
      "cont13    0.380742\n",
      "cont14    0.248674\n",
      "loss      3.794958\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of training data is: \")\n",
    "print train.shape\n",
    "print (\"Description of training data is: \")\n",
    "print train.describe()\n",
    "print (\"Skewness of training data is: \")\n",
    "print train.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that loss has the largest variance and skewness as well. Reduce the skewness by implementing data transformation. Let's take log of the loss column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[\"loss\"] = np.log(train[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhMAAAFYCAYAAADp1H4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noShMiIiISCj/P4EfHJXO89kOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104714850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of transformed loss now is: \n",
      "0.0929745455514\n"
     ]
    }
   ],
   "source": [
    "# visualize the transformed column\n",
    "sns.violinplot(data=train,y=\"loss\")  \n",
    "plt.show()\n",
    "print (\"Skewness of transformed loss now is: \")\n",
    "print train[\"loss\"].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert string value to numeric value\n",
    "def encode(data):\n",
    "    \"\"\"Encode categorical integer features using one-of-K scheme.\n",
    "    The output will be a sparse matrix where each column corresponds to one possible value of one feature.\"\"\"\n",
    "    \n",
    "    # remove id column\n",
    "    reduced_data = data.iloc[:,1:]\n",
    "    labels = []\n",
    "    cats = []\n",
    "    cols = reduced_data.columns\n",
    "    \n",
    "    for i in range(0, SPLIT):\n",
    "        reduced_labels = reduced_data[cols[i]].unique()\n",
    "        labels.append(list(set(reduced_labels)))    \n",
    "\n",
    "    \n",
    "    for i in range(0, SPLIT):\n",
    "        #Label encode\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(labels[i])\n",
    "        feature = label_encoder.transform(reduced_data.iloc[:,i])\n",
    "        feature = feature.reshape(reduced_data.shape[0], 1)\n",
    "        #One hot encode\n",
    "        onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n",
    "        feature = onehot_encoder.fit_transform(feature)\n",
    "        cats.append(feature)\n",
    "\n",
    "    # Make a 2D array from a list of 1D arrays\n",
    "    encoded_cats = np.column_stack(cats)\n",
    "    encoded_data = np.concatenate((encoded_cats, reduced_data.iloc[:,SPLIT:].values),axis=1)\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = encode(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train[:, -1]\n",
    "train_features = train[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# seperate training and testing data by \n",
    "features_train, features_test, labels_train, labels_test = train_test_split(train_features, train_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a regression problem since we need to predict on the loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def meanAbsoluteError(predictedY, actualY):\n",
    "    \"\"\"Return the mean absolute error of a algorithm.\"\"\"\n",
    "    #answer = np.array(answer)\n",
    "    #prediction = np.array(predictedY)\n",
    "    error = predictedY - actualY\n",
    "    return np.sum(np.absolute(error))/error.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try several regression algorithms to find a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least square estimation solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-5f0017fd0f2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/lordlavon/anaconda/lib/python2.7/site-packages/numpy/linalg/linalg.pyc\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lordlavon/anaconda/lib/python2.7/site-packages/numpy/linalg/linalg.pyc\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "weight = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(train_features), train_features)), np.transpose(train_features)),train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.transpose(train_features) * train_features is singular matrix. Let's try gradient descent instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeError(weights, X, train_labels):\n",
    "    squaredError = (np.dot(X, weights)-train_labels)**2\n",
    "    error = (1.0 / (2 * train_labels.shape[0])) * np.sum(squaredError)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescent(train_features, train_labels, alpha, num_iters):\n",
    "    weights = np.zeros((train_features.shape[1]+1, 1))\n",
    "    \n",
    "    one = np.ones((train_features.shape[0], 1))\n",
    "    X = np.concatenate((train_features, one), axis = 1)\n",
    "    \n",
    "    seList = np.zeros((num_iters, 1))\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        predictions = np.dot(X, weights)\n",
    "\n",
    "        errors = (predictions - train_labels) * X[:, ]\n",
    "        #errors_x2 = (predictions - y) * X[:, 1]\n",
    "\n",
    "        #theta[0][0] = theta[0][0] - alpha * (1.0 / m) * errors_x1.sum()\n",
    "        weights = weights - alpha * (1.0 / train_labels.shape[0]) * np.sum(errors)\n",
    "\n",
    "        seList[i, 0] = computeError(weights, X, train_labels)\n",
    "\n",
    "    return weights, seList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_train_LR = labels_train.reshape(labels_train.shape[0], 1)\n",
    "labels_test_LR = labels_test.reshape(labels_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights, seList = gradientDescent(features_train, labels_train_LR, 0.005, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.69507399e+005],\n",
       "       [  9.73034793e+008],\n",
       "       [  5.58558833e+012],\n",
       "       [  3.20633931e+016],\n",
       "       [  1.84056023e+020],\n",
       "       [  1.05655130e+024],\n",
       "       [  6.06500472e+027],\n",
       "       [  3.48154248e+031],\n",
       "       [  1.99853727e+035],\n",
       "       [  1.14723611e+039],\n",
       "       [  6.58556988e+042],\n",
       "       [  3.78036659e+046],\n",
       "       [  2.17007364e+050],\n",
       "       [  1.24570448e+054],\n",
       "       [  7.15081563e+057],\n",
       "       [  4.10483907e+061],\n",
       "       [  2.35633313e+065],\n",
       "       [  1.35262449e+069],\n",
       "       [  7.76457698e+072],\n",
       "       [  4.45716134e+076],\n",
       "       [  2.55857946e+080],\n",
       "       [  1.46872154e+084],\n",
       "       [  8.43101804e+087],\n",
       "       [  4.83972375e+091],\n",
       "       [  2.77818478e+095],\n",
       "       [  1.59478331e+099],\n",
       "       [  9.15466038e+102],\n",
       "       [  5.25512187e+106],\n",
       "       [  3.01663904e+110],\n",
       "       [  1.73166509e+114],\n",
       "       [  9.94041363e+117],\n",
       "       [  5.70617400e+121],\n",
       "       [  3.27556005e+125],\n",
       "       [  1.88029556e+129],\n",
       "       [  1.07936088e+133],\n",
       "       [  6.19594037e+136],\n",
       "       [  3.55670450e+140],\n",
       "       [  2.04168312e+144],\n",
       "       [  1.17200346e+148],\n",
       "       [  6.72774386e+151],\n",
       "       [  3.86197985e+155],\n",
       "       [  2.21692274e+159],\n",
       "       [  1.27259764e+163],\n",
       "       [  7.30519255e+166],\n",
       "       [  4.19345727e+170],\n",
       "       [  2.40720334e+174],\n",
       "       [  1.38182591e+178],\n",
       "       [  7.93220421e+181],\n",
       "       [  4.55338572e+185],\n",
       "       [  2.61381590e+189],\n",
       "       [  1.50042935e+193],\n",
       "       [  8.61303288e+196],\n",
       "       [  4.94420717e+200],\n",
       "       [  2.83816222e+204],\n",
       "       [  1.62921263e+208],\n",
       "       [  9.35229772e+211],\n",
       "       [  5.36857319e+215],\n",
       "       [  3.08176440e+219],\n",
       "       [  1.76904951e+223],\n",
       "       [  1.01550144e+227],\n",
       "       [  5.82936296e+230],\n",
       "       [  3.34627518e+234],\n",
       "       [  1.92088872e+238],\n",
       "       [  1.10266290e+242],\n",
       "       [  6.32970276e+245],\n",
       "       [  3.63348918e+249],\n",
       "       [  2.08576044e+253],\n",
       "       [  1.19730551e+257],\n",
       "       [  6.87298720e+260],\n",
       "       [  3.94535503e+264],\n",
       "       [  2.26478325e+268],\n",
       "       [  1.30007138e+272],\n",
       "       [  7.46290226e+275],\n",
       "       [  4.28398862e+279],\n",
       "       [  2.45917176e+283],\n",
       "       [  1.41165775e+287],\n",
       "       [  8.10345030e+290],\n",
       "       [  4.65168746e+294],\n",
       "       [  2.67024483e+298],\n",
       "       [  1.53282169e+302],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf],\n",
       "       [              inf]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -5.48995676e+186],\n",
       "       [ -5.48995676e+186],\n",
       "       [ -5.48995676e+186],\n",
       "       ..., \n",
       "       [ -5.48995676e+186],\n",
       "       [ -5.48995676e+186],\n",
       "       [ -5.48995676e+186]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.80248764211e+188\n"
     ]
    }
   ],
   "source": [
    "# since we have noticed that the value of weights are the same, lets use partition of weights to fit the dimension of test data\n",
    "one = np.ones((features_test.shape[0], 1))\n",
    "X = np.concatenate((features_test, one), axis = 1)\n",
    "predictedY = np.dot(X, weights)\n",
    "\n",
    "error = meanAbsoluteError(predictedY, labels_test_LR)\n",
    "print error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbor (Non Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=2)\n",
    "knn.fit(features_train, labels_train) \n",
    "predictedY = knn.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.579777626158\n"
     ]
    }
   ],
   "source": [
    "print meanAbsoluteError(predictedY, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced solution by Ekaterina Aladyeva on Kaggle using xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_X = features_train.as_matrix()\n",
    "#test_X = features_test.as_matrix()\n",
    "#train_y = labels_train.as_matrix()\n",
    "\n",
    "\n",
    "gbm = xgboost.XGBRegressor(max_depth=5, n_estimators=300, learning_rate=0.05).fit(features_train, labels_train)\n",
    "predictedY = gbm.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.421825221453\n"
     ]
    }
   ],
   "source": [
    "error = meanAbsoluteError(predictedY, labels_test)\n",
    "print error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose to improve the algorithm by cross validation and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoring_fnc = make_scorer(meanAbsoluteError, greater_is_better=False)\n",
    "tuned_parameters={'max_depth': [2, 5, 8], 'n_estimators': [100, 200, 300, 500]}\n",
    "gbm = GridSearchCV(xgboost.XGBRegressor(learning_rate = 0.05), tuned_parameters, cv=5, scoring= scoring_fnc)\n",
    "gbm.fit(features_train, labels_train)\n",
    "predictedY = gbm.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = meanAbsoluteError(predictedY, labels_test)\n",
    "print error"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
